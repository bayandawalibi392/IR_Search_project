# faiss_search_webis.py

# import faiss
# import joblib
# import numpy as np
# from sentence_transformers import SentenceTransformer
# from TextPreprocessor import TextPreprocessor
# import sqlite3
# import textwrap
# import time

# # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# GROUP = 'webis'
# MODEL_DIR = 'models'
# INDEX_DIR = 'indexes'
# DB_PATH = 'ir_project.db'
# TOP_K = 10
# MODEL_NAME = 'all-MiniLM-L6-v2'

# # ØªØ­Ù…ÙŠÙ„ ÙÙ‡Ø±Ø³ FAISS Ø§Ù„Ù…Ø­ÙÙˆØ¸ Ù…Ø³Ø¨Ù‚Ù‹Ø§
# print("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ ÙÙ‡Ø±Ø³ FAISS...")
# index = faiss.read_index(f"{INDEX_DIR}/faiss_index_{GROUP}_bert.index")
# print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ø¹Ø¯Ø¯: {index.ntotal} Ù…ØªØ¬Ù‡")

# # ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
# doc_ids = joblib.load(f"{MODEL_DIR}/doc_ids_bert_{GROUP}.joblib")

# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
# bert_model = SentenceTransformer(MODEL_NAME)
# pre = TextPreprocessor()

# # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# conn = sqlite3.connect(DB_PATH)
# cursor = conn.cursor()

# # Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
# while True:
#     query = input("\nğŸ” Ø£Ø¯Ø®Ù„ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ùƒ (Ø£Ùˆ 'exit' Ù„Ù„Ø®Ø±ÙˆØ¬): ").strip()
#     if query.lower() == 'exit':
#         break

#     # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
#     tokens = pre.preprocess(query)
#     if not tokens:
#         print("âš ï¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
#         continue

#     query_text = ' '.join(tokens)
#     query_vec = bert_model.encode([query_text]).astype('float32')

#     # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ FAISS
#     start = time.perf_counter()
#     D, I = index.search(query_vec, TOP_K)
#     end = time.perf_counter()

#     print(f"\nâ±ï¸ Ø²Ù…Ù† Ø§Ù„ØªÙ†ÙÙŠØ°: {end - start:.4f} Ø«Ø§Ù†ÙŠØ©")
#     print(f"\nğŸ“„ Ø£Ø¹Ù„Ù‰ {TOP_K} Ù†ØªØ§Ø¦Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FAISS + BERT:")

#     for rank, (idx, dist) in enumerate(zip(I[0], D[0]), 1):
#         doc_id = doc_ids[idx]

#         cursor.execute("SELECT content FROM documents WHERE doc_id = ? AND source = ?", (doc_id, GROUP))
#         row = cursor.fetchone()
#         content = row[0] if row else "(Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ)"

#         print(f"#{rank} | ğŸ”‘ ID: {doc_id} | ğŸ§® Distance: {dist:.4f}")
#         print(textwrap.shorten(content, width=300))
#         print()
from flask import Flask, request, jsonify
from flask_cors import CORS
import sqlite3, joblib, numpy as np, time, textwrap, requests
from sentence_transformers import SentenceTransformer
import faiss

app = Flask(__name__)
CORS(app)

# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
GROUP = 'webis'
MODEL_NAME = 'all-MiniLM-L6-v2'
MODEL_DIR = 'models'
INDEX_DIR = 'indexes'
DB_PATH = 'ir_project.db'
TOP_K = 10
PREPROCESS_API_URL = "http://localhost:5050/preprocess"

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ùˆ Ø§Ù„ÙÙ‡Ø±Ø³
print("ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BERT ÙˆØ§Ù„ÙÙ‡Ø±Ø³...")
bert_model = SentenceTransformer(MODEL_NAME)
doc_ids = joblib.load(f"{MODEL_DIR}/doc_ids_bert_{GROUP}.joblib")
faiss_index = faiss.read_index(f"{INDEX_DIR}/faiss_index_{GROUP}_bert.index")

# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect(DB_PATH, check_same_thread=False)
cursor = conn.cursor()

@app.route('/search-faiss', methods=['POST'])
def search_faiss():
    data = request.get_json()
    query_text = data.get('query', '').strip()
    if not query_text:
        return jsonify({"error": "âš ï¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙØ§Ø±Øº"}), 400

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ø¨Ø± Ø®Ø¯Ù…Ø© Ø®Ø§Ø±Ø¬ÙŠØ©
    try:
        response = requests.post(PREPROCESS_API_URL, json={
            "text": query_text,
            "use_stemming": True,
            "use_lemmatization": False
        })
        if response.status_code != 200:
            return jsonify({"error": "âš ï¸ ÙØ´Ù„ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ©"}), 500

        tokens = response.json().get("tokens", [])
    except Exception as e:
        return jsonify({"error": f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"}), 500

    if not tokens:
        return jsonify({"error": "âš ï¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"}), 400

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ BERT
    query_cleaned = ' '.join(tokens)
    query_vec = bert_model.encode([query_cleaned]).astype('float32')

    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ FAISS
    start_time = time.perf_counter()
    D, I = faiss_index.search(query_vec, TOP_K)
    elapsed_time = time.perf_counter() - start_time

    results = []
    for rank, (idx, dist) in enumerate(zip(I[0], D[0]), 1):
        doc_id = doc_ids[idx]
        cursor.execute("SELECT content FROM documents WHERE doc_id = ? AND source = ?", (doc_id, GROUP))
        row = cursor.fetchone()
        content = row[0] if row else "(Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ)"

        results.append({
            "rank": rank,
            "doc_id": doc_id,
            "score": round(float(1 - dist), 4),  # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¥Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡
            "content": textwrap.shorten(content, width=300)
        })

    return jsonify({
        "query": query_text,
        "execution_time": round(float(elapsed_time), 4),
        "results": results
    })

if __name__ == '__main__':
    app.run(debug=True, port=5020)
