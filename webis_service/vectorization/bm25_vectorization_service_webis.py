import os
import sqlite3
import joblib
from rank_bm25 import BM25Okapi
from TextPreprocessor import TextPreprocessor
from tqdm import tqdm  # âœ… Ø¥Ø¶Ø§ÙØ© tqdm

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
DB_PATH = 'ir_project.db'
OUTPUT_DIR = 'models'
GROUPS = ['webis']
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

# ÙƒØ§Ø¦Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
pre = TextPreprocessor()

def build_bm25_for_group(group_name):
    cursor.execute("SELECT doc_id, content FROM preprocessed_documents WHERE source = ?", (group_name,))
    rows = cursor.fetchall()

    doc_ids = []
    tokenized_documents = []

    print(f"ğŸ”„ Processing {len(rows)} documents from group '{group_name}'...")

    # âœ… Ø¥Ø¶Ø§ÙØ© tqdm Ù‡Ù†Ø§ Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø¯Ù…
    for doc_id, content_str in tqdm(rows, desc="ğŸ” Tokenizing documents"):
        if content_str:
            doc_ids.append(doc_id)
            tokens = pre.tokenize(pre.clean_text(content_str))
            tokenized_documents.append(tokens)

    print(f"âœ… Loaded {len(tokenized_documents)} preprocessed documents from group '{group_name}'")

    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ BM25
    bm25 = BM25Okapi(tokenized_documents)

    # Ø§Ù„ØªØ®Ø²ÙŠÙ†
    joblib.dump(bm25, f"{OUTPUT_DIR}/bm25_model_{group_name}.joblib")
    joblib.dump(doc_ids, f"{OUTPUT_DIR}/doc_ids_bm25_{group_name}.joblib")
    joblib.dump(tokenized_documents, f"{OUTPUT_DIR}/bm25_tokenized_docs_{group_name}.joblib")

    print(f"ğŸ’¾ Saved BM25 model for group '{group_name}'")

# ØªÙ†ÙÙŠØ°
for group in GROUPS:
    build_bm25_for_group(group)

print("âœ… BM25 vectorization completed for all documents per group.")
