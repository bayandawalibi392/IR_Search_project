import os
import sqlite3
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from TextPreprocessor import TextPreprocessor  # Ù„Ø§ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ù‡Ù†Ø§

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
DB_PATH = 'ir_project.db'
OUTPUT_DIR = 'models'
GROUPS = ['webis']
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

# ÙƒØ§Ø¦Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
pre = TextPreprocessor()

def build_tfidf_for_group(group_name):
    cursor.execute("SELECT doc_id, content FROM preprocessed_documents WHERE source = ?", (group_name,))
    rows = cursor.fetchall()

    doc_ids = []
    processed_texts = []

    for doc_id, content_str in rows:
        if content_str:
            doc_ids.append(doc_id)

            # âš ï¸ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ÙŠØ¯ÙˆÙŠØ§Ù‹ØŒ Ø¨Ø¯ÙˆÙ† ØªÙ…Ø±ÙŠØ±Ù‡ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù€ Vectorizer
            tokens = pre.preprocess(content_str, use_stemming=True, use_lemmatization=False)
            clean_text = ' '.join(tokens)
            processed_texts.append(clean_text)

    print(f"âœ… Loaded and processed {len(processed_texts)} documents from group '{group_name}'")

    # âœ… Ø¥Ù†Ø´Ø§Ø¡ vectorizer Ø¨Ø¯ÙˆÙ† preprocessor/tokenizer
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(processed_texts)

    # âœ… Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø¨Ø¯ÙˆÙ† ØªØ¶Ù…ÙŠÙ† Ø£ÙŠ ÙƒØ§Ø¦Ù† Ù…Ø®ØµØµ
    joblib.dump(vectorizer, f"{OUTPUT_DIR}/tfidf_vectorizer_{group_name}.joblib")
    joblib.dump(tfidf_matrix, f"{OUTPUT_DIR}/tfidf_vectors_{group_name}.joblib")
    joblib.dump(doc_ids, f"{OUTPUT_DIR}/doc_ids_{group_name}.joblib")
    print(f"ğŸ’¾ Saved TF-IDF model for group '{group_name}'")

# ØªÙ†ÙÙŠØ°
for group in GROUPS:
    build_tfidf_for_group(group)

print("âœ… TF-IDF vectorization completed and saved WITHOUT custom objects.")
