# import sqlite3
# import joblib
# import numpy as np
# import time
# from sklearn.metrics import average_precision_score
# from sklearn.metrics.pairwise import cosine_similarity
# from TextPreprocessor import TextPreprocessor
# from sentence_transformers import SentenceTransformer
# import warnings

# # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# DB_PATH = 'ir_project.db'
# MODEL_DIR = 'models'
# INDEX_DIR = 'indexes'
# GROUP = 'webis'  # ÙÙ‚Ø· Ù…Ø¬Ù…ÙˆØ¹Ø© ÙˆØ§Ø­Ø¯Ø©
# TOP_K = 10
# ALPHA = 0.7  # Ù†Ø³Ø¨Ø© Ø¯Ù…Ø¬ TF-IDF Ùˆ BERT

# print(f"ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: {GROUP}...")

# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
# tfidf_vectorizer = joblib.load(f"{MODEL_DIR}/tfidf_vectorizer_{GROUP}.joblib")
# tfidf_matrix = joblib.load(f"{MODEL_DIR}/tfidf_vectors_{GROUP}.joblib")
# tfidf_doc_ids = joblib.load(f"{MODEL_DIR}/doc_ids_{GROUP}.joblib")

# bert_vectors = joblib.load(f"{MODEL_DIR}/bert_vectors_{GROUP}.joblib")
# bert_doc_ids = joblib.load(f"{MODEL_DIR}/doc_ids_bert_{GROUP}.joblib")

# # Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ bert_vectors Ù„ÙŠØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ ØªØ±ØªÙŠØ¨ tfidf_doc_ids
# bert_id_to_vec = dict(zip(bert_doc_ids, bert_vectors))
# bert_vectors_aligned = np.array([bert_id_to_vec[doc_id] for doc_id in tfidf_doc_ids])

# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¹ÙƒØ³ÙŠ
# inverted_index = joblib.load(f"{INDEX_DIR}/inverted_index1_{GROUP}.joblib")

# # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# conn = sqlite3.connect(DB_PATH)
# cursor = conn.cursor()

# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
# cursor.execute("SELECT query_id, query_text FROM queries WHERE source = ?", (GROUP,))
# queries = cursor.fetchall()
# query_dict = {q_id: text for q_id, text in queries}

# cursor.execute("SELECT query_id, doc_id FROM qrels WHERE source = ?", (GROUP,))
# qrel_rows = cursor.fetchall()
# qrels = {}
# for q_id, doc_id in qrel_rows:
#     qrels.setdefault(q_id, set()).add(doc_id)

# # Ø§Ù„ØªØ­Ø¶ÙŠØ±
# pre = TextPreprocessor()
# bert_model = SentenceTransformer('all-MiniLM-L6-v2')

# # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
# map_scores, mrr_scores, recall_scores, precision_scores = [], [], [], []

# start_time = time.perf_counter()

# for query_id, query_text in query_dict.items():
#     tokens = pre.preprocess(query_text, use_stemming=True, use_lemmatization=False)
#     if not tokens:
#         continue

#     query_str = pre.clean_text(' '.join(tokens))
#     query_vec_tfidf = tfidf_vectorizer.transform([query_str])
#     query_vec_bert = bert_model.encode([' '.join(tokens)]).reshape(1, -1)

#     # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¹ÙƒØ³ÙŠ
#     candidate_indices = set()
#     for token in tokens:
#         if token in inverted_index:
#             candidate_indices.update(inverted_index[token])
#     if not candidate_indices:
#         continue

#     candidate_indices = sorted(candidate_indices)
#     candidate_tfidf = tfidf_matrix[candidate_indices]
#     candidate_bert = bert_vectors_aligned[candidate_indices]
#     candidate_doc_ids = [tfidf_doc_ids[i] for i in candidate_indices]

#     # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
#     sim_tfidf = cosine_similarity(query_vec_tfidf, candidate_tfidf)[0]
#     sim_bert = cosine_similarity(query_vec_bert, candidate_bert)[0]
#     scores = ALPHA * sim_tfidf + (1 - ALPHA) * sim_bert

#     top_indices = np.argsort(scores)[-TOP_K:][::-1]
#     retrieved = [candidate_doc_ids[i] for i in top_indices]
#     y_scores = scores[top_indices]

#     relevant_docs = qrels.get(query_id, set())
#     y_true = [1 if doc_id in relevant_docs else 0 for doc_id in retrieved]

#     # MAP
#     if any(y_true):
#         with warnings.catch_warnings():
#             warnings.simplefilter("ignore")
#             map_scores.append(average_precision_score(y_true, y_scores))
#     else:
#         map_scores.append(0)

#     # MRR
#     for rank, rel in enumerate(y_true, 1):
#         if rel:
#             mrr_scores.append(1 / rank)
#             break
#     else:
#         mrr_scores.append(0)

#     # Recall@K
#     recall = sum(y_true) / len(relevant_docs) if relevant_docs else 0
#     recall_scores.append(recall)

#     # Precision@K
#     precision = sum(y_true) / len(y_true) if y_true else 0
#     precision_scores.append(precision)

# end_time = time.perf_counter()
# elapsed = end_time - start_time

# # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
# print("\nâœ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù…ÙƒØªÙ…Ù„!")
# print(f"ğŸ“Š MAP: {np.mean(map_scores):.4f}")
# print(f"ğŸ“Š MRR: {np.mean(mrr_scores):.4f}")
# print(f"ğŸ“Š Recall@{TOP_K}: {np.mean(recall_scores):.4f}")
# print(f"ğŸ“Š Precision@{TOP_K}: {np.mean(precision_scores):.4f}")
# print(f"ğŸ•’ Ø²Ù…Ù† Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒÙ„ÙŠ: {elapsed:.2f} Ø«Ø§Ù†ÙŠØ©")
import sqlite3
import joblib
import numpy as np
import time
import warnings
import pandas as pd
import json
from sklearn.metrics import average_precision_score
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from TextPreprocessor import TextPreprocessor
from pprint import pprint

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
DB_PATH = 'ir_project.db'
MODEL_DIR = 'models'
INDEX_DIR = 'indexes'
GROUP = 'webis'
TOP_K = 10
ALPHA = 0.7  # Ø§Ù„Ù†Ø³Ø¨Ø© Ø¨ÙŠÙ† TF-IDF ÙˆBERT

print(f"ğŸ“¦ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: {GROUP}...")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
tfidf_vectorizer = joblib.load(f"{MODEL_DIR}/tfidf_vectorizer_{GROUP}.joblib")
tfidf_matrix = joblib.load(f"{MODEL_DIR}/tfidf_vectors_{GROUP}.joblib")
tfidf_doc_ids = joblib.load(f"{MODEL_DIR}/doc_ids_{GROUP}.joblib")

bert_vectors = joblib.load(f"{MODEL_DIR}/bert_vectors_{GROUP}.joblib")
bert_doc_ids = joblib.load(f"{MODEL_DIR}/doc_ids_bert_{GROUP}.joblib")

# Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ bert_vectors Ø¨Ø­Ø³Ø¨ ØªØ±ØªÙŠØ¨ tfidf_doc_ids
bert_id_to_vec = dict(zip(bert_doc_ids, bert_vectors))
bert_vectors_aligned = np.array([bert_id_to_vec[doc_id] for doc_id in tfidf_doc_ids])

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø¹ÙƒØ³ÙŠ
inverted_index = joblib.load(f"{INDEX_DIR}/inverted_index1_{GROUP}.joblib")

# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª
cursor.execute("SELECT query_id, query_text FROM queries WHERE source = ?", (GROUP,))
queries = cursor.fetchall()
query_dict = {q_id: text for q_id, text in queries}

cursor.execute("SELECT query_id, doc_id FROM qrels WHERE source = ?", (GROUP,))
qrel_rows = cursor.fetchall()
qrels = {}
for q_id, doc_id in qrel_rows:
    qrels.setdefault(q_id, set()).add(doc_id)

# Ø§Ù„ØªØ­Ø¶ÙŠØ±
pre = TextPreprocessor()
bert_model = SentenceTransformer('all-MiniLM-L6-v2')

# Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
map_scores, mrr_scores, recall_scores, precision_scores = [], [], [], []

start_time = time.perf_counter()

for query_id, query_text in query_dict.items():
    tokens = pre.preprocess(query_text, use_stemming=True, use_lemmatization=False)
    if not tokens:
        continue

    query_str = pre.clean_text(' '.join(tokens))
    query_vec_tfidf = tfidf_vectorizer.transform([query_str])
    query_vec_bert = bert_model.encode([' '.join(tokens)]).reshape(1, -1)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†
    candidate_indices = set()
    for token in tokens:
        if token in inverted_index:
            candidate_indices.update(inverted_index[token])
    if not candidate_indices:
        continue

    candidate_indices = sorted(candidate_indices)
    candidate_tfidf = tfidf_matrix[candidate_indices]
    candidate_bert = bert_vectors_aligned[candidate_indices]
    candidate_doc_ids = [tfidf_doc_ids[i] for i in candidate_indices]

    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    sim_tfidf = cosine_similarity(query_vec_tfidf, candidate_tfidf)[0]
    sim_bert = cosine_similarity(query_vec_bert, candidate_bert)[0]
    scores = ALPHA * sim_tfidf + (1 - ALPHA) * sim_bert

    top_indices = np.argsort(scores)[-TOP_K:][::-1]
    retrieved = [candidate_doc_ids[i] for i in top_indices]
    y_scores = scores[top_indices]

    relevant_docs = qrels.get(query_id, set())
    y_true = [1 if doc_id in relevant_docs else 0 for doc_id in retrieved]

    # MAP
    if any(y_true):
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            map_scores.append(average_precision_score(y_true, y_scores))
    else:
        map_scores.append(0)

    # MRR
    for rank, rel in enumerate(y_true, 1):
        if rel:
            mrr_scores.append(1 / rank)
            break
    else:
        mrr_scores.append(0)

    # Recall@K
    recall = sum(y_true) / len(relevant_docs) if relevant_docs else 0
    recall_scores.append(recall)

    # Precision@K
    precision = sum(y_true) / len(y_true) if y_true else 0
    precision_scores.append(precision)

elapsed = time.perf_counter() - start_time

# Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
results = {
    "MAP": round(np.mean(map_scores), 4),
    "MRR": round(np.mean(mrr_scores), 4),
    f"Recall@{TOP_K}": round(np.mean(recall_scores), 4),
    f"Precision@{TOP_K}": round(np.mean(precision_scores), 4),
    "Execution Time (s)": round(elapsed, 2),
    "Queries Evaluated": len(map_scores)
}

# Ø·Ø¨Ø§Ø¹Ø© ÙˆØ¬Ø¯ÙˆÙ„
print("\nâœ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù…ÙƒØªÙ…Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Hybrid (TF-IDF + BERT)!")
pprint(results)

df_results = pd.DataFrame([results])
display(df_results)  # Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¯Ø§Ø®Ù„ Jupyter

# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
df_results.to_csv("hybrid_evaluation_results.csv", index=False)
with open("hybrid_evaluation_results.json", "w", encoding="utf-8") as f:
    json.dump(results, f, indent=4, ensure_ascii=False)
