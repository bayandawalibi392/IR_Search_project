import os
import sqlite3
import joblib
import nltk
from nltk.tokenize import word_tokenize
from collections import defaultdict
from tqdm import tqdm

# ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª NLTK
nltk.download('punkt')

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
DB_PATH = "ir_project.db"
INDEX_DIR = "indexes"
SOURCES = ['quora']
os.makedirs(INDEX_DIR, exist_ok=True)

# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

for source in SOURCES:
    print(f"\nğŸ“š Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {source}")

    # Ø¬Ù„Ø¨ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    cursor.execute("SELECT doc_id, content FROM preprocessed_documents WHERE source = ?", (source,))
    rows = cursor.fetchall()
    if not rows:
        print(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„Ù„Ù…ØµØ¯Ø± {source}ØŒ ØªØ®Ø·ÙŠ...")
        continue

    inverted_index = defaultdict(set)  # term â†’ set of doc_ids

    print(f"ğŸ“¥ Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ Ù„Ù€ {len(rows)} Ù…Ø³ØªÙ†Ø¯...")
    for doc_id, content in tqdm(rows, desc=f"Indexing {source}"):
        if not content.strip():
            continue
        tokens = word_tokenize(content.lower())
        for token in set(tokens):  # Ù†ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø¯Ø§Ø®Ù„ Ù†ÙØ³ Ø§Ù„Ù…Ø³ØªÙ†Ø¯
            inverted_index[token].add(doc_id)

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø¥Ù„Ù‰ Ù‚ÙˆØ§Ø¦Ù…
    inverted_index = {term: list(doc_ids) for term, doc_ids in inverted_index.items()}

    # Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ ÙÙŠ Ù…Ø¬Ù„Ø¯ indexes
    index_path = os.path.join(INDEX_DIR, f"inverted_index_{source}.joblib")
    joblib.dump(inverted_index, index_path)
    print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ÙÙ‡Ø±Ø³ ÙÙŠ: {index_path}")

conn.close()
print("\nğŸ Ø§Ù†ØªÙ‡Ù‰ Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³Ø©.")
