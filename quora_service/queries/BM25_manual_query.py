# import os
# import sqlite3
# import joblib
# from nltk.tokenize import word_tokenize
# from rank_bm25 import BM25Okapi
# from text_preprocessing_service import TextPreprocessingService
# from tqdm import tqdm

# # --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# MODELS_DIR = "models"
# SOURCE = "quora"  # Ø£Ùˆ "quora"
# TOP_N = 10

# # Ø±Ø¨Ø· Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# conn = sqlite3.connect("ir_project.db")
# cursor = conn.cursor()

# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
# cursor.execute("SELECT query_id, query_text FROM queries WHERE source = ?", (SOURCE,))
# all_queries = cursor.fetchall()

# print(f"\nğŸ” Ø§Ø®ØªØ± Ø§Ø³ØªØ¹Ù„Ø§Ù…Ù‹Ø§ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© {SOURCE.upper()}:")
# for i, (qid, qtext) in enumerate(all_queries[:10]):
#     print(f"{i+1}. {qtext}")

# index = int(input("\nğŸ“Œ Ø£Ø¯Ø®Ù„ Ø±Ù‚Ù… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (1-10): ")) - 1
# query_id, query_text = all_queries[index]

# print(f"\nğŸ§  ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query_text} (ID: {query_id})")

# # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BM25
# model_path = os.path.join(MODELS_DIR, f"bm25_{SOURCE}_model.joblib")
# bm25_data = joblib.load(model_path)
# doc_ids = bm25_data['doc_ids']
# tokenized_docs = bm25_data['tokenized_texts']
# bm25 = BM25Okapi(tokenized_docs, k1=bm25_data['k1'], b=bm25_data['b'])

# # ØªÙ‡ÙŠØ¦Ø© Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ©
# preprocessor = TextPreprocessingService()
# cleaned_query = preprocessor.preprocess(query_text, return_as_string=True)
# tokenized_query = word_tokenize(cleaned_query)

# # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª BM25 Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
# scores = bm25.get_scores(tokenized_query)
# top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:TOP_N]

# print(f"\nğŸ“„ Ø£Ø¹Ù„Ù‰ {TOP_N} Ù†ØªØ§Ø¦Ø¬ Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BM25:\n")

# for rank, idx in enumerate(top_indices, 1):
#     doc_id = doc_ids[idx]
#     score = scores[idx]

#     # Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
#     cursor.execute("SELECT content FROM documents WHERE doc_id = ?", (doc_id,))
#     result = cursor.fetchone()
#     content = result[0] if result else "(Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ)"

#     print(f"{rank}. doc_id: {doc_id}")
#     print(f"   BM25 score: {score:.4f}")
#     print(f"   content: {content[:300]}...")  # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 300 Ø­Ø±Ù ÙÙ‚Ø·
#     print("-" * 80)

# conn.close()

from flask import Flask, request, jsonify
import os
import sqlite3
import joblib
from nltk.tokenize import word_tokenize
from rank_bm25 import BM25Okapi
import requests

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
MODELS_DIR = "models"
SOURCE = "quora"  # Ù…Ù…ÙƒÙ† ØªØ¹Ø¯Ù„ Ø§Ù„Ù…ØµØ¯Ø± Ø­Ø³Ø¨ Ø­Ø§Ø¬ØªÙƒ
TOP_N = 10
PREPROCESS_API_URL = "http://127.0.0.1:5060/preprocess"  # Ø±Ø§Ø¨Ø· Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©

# Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ Flask
app = Flask(__name__)

# ÙØªØ­ Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect("ir_project.db", check_same_thread=False)
cursor = conn.cursor()

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BM25 Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
model_path = os.path.join(MODELS_DIR, f"bm25_{SOURCE}_model.joblib")
bm25_data = joblib.load(model_path)
doc_ids = bm25_data['doc_ids']
tokenized_docs = bm25_data['tokenized_texts']
bm25 = BM25Okapi(tokenized_docs, k1=bm25_data['k1'], b=bm25_data['b'])

@app.route("/search-bm25", methods=["POST"])
def search_bm25():
    data = request.get_json()
    query = data.get("query", "").strip()

    if not query:
        return jsonify({"error": "âš ï¸ ÙŠØ¬Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ø­Ù‚Ù„ 'query' ÙÙŠ Ø§Ù„Ø·Ù„Ø¨"}), 400

    # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
    try:
        response = requests.post(PREPROCESS_API_URL, json={
            "text": query,
            "return_as_string": False  # Ù†Ø±ÙŠØ¯ Ø§Ù„ØªÙˆÙƒÙ†Ø² ÙƒÙ‚Ø§Ø¦Ù…Ø©
        })
        if response.status_code != 200:
            return jsonify({"error": "âš ï¸ ÙØ´Ù„ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©"}), 500
        tokens = response.json().get("tokens", [])
    except Exception as e:
        return jsonify({"error": f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"}), 500

    if not tokens:
        return jsonify({"error": "âš ï¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"}), 400

    # Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª BM25
    scores = bm25.get_scores(tokens)
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:TOP_N]

    results = []
    for rank, idx in enumerate(top_indices, 1):
        doc_id = doc_ids[idx]
        score = scores[idx]

        # Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        cursor.execute("SELECT content FROM documents WHERE doc_id = ?", (doc_id,))
        result = cursor.fetchone()
        content = result[0] if result else "(Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ)"

        results.append({
            "rank": rank,
            "doc_id": doc_id,
            "score": round(float(score), 4),
            "content": content[:500]  # Ø£ÙˆÙ„ 500 Ø­Ø±Ù
        })

    return jsonify({
        "query": query,
        "tokens": tokens,
        "top_n": TOP_N,
        "results": results
    })


if __name__ == "__main__":
    app.run(port=5017, debug=True)
