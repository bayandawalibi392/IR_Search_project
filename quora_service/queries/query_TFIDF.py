# import joblib
# import os
# import sqlite3
# import numpy as np
# import time
# from sklearn.metrics.pairwise import cosine_similarity
# from text_preprocessing_service import TextPreprocessingService
# # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# INDEX_DIR = 'indexes'
# MODELS_DIR = 'models'
# TOP_N = 10
# SOURCE = "quora"  # Ø£Ùˆ "quora"

# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³
# index_path = os.path.join(INDEX_DIR, f"inverted_index_{SOURCE}.joblib")
# print(f"ðŸ“‚ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³ Ù…Ù†: {index_path}")
# inverted_index = joblib.load(index_path)

# # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ TF-IDF
# tfidf_vectorizer = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_vectorizer.joblib"))
# doc_ids = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_doc_ids.joblib"))
# doc_matrix = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_matrix.joblib"))

# # Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ©
# preprocessor = TextPreprocessingServiceWebis()

# # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ù…ÙˆØ³ doc_id Ø¥Ù„Ù‰ index
# doc_id_to_idx = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}

# def search_tfidf(query):
#     tokens = preprocessor.preprocess(query, return_as_string=False)
#     print(f"âœ… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ (tokens): {tokens}")

#     # Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³
#     candidate_doc_ids = set()
#     for term in tokens:
#         if term in inverted_index:
#             candidate_doc_ids.update(inverted_index[term])

#     if not candidate_doc_ids:
#         print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙˆØ«Ø§Ø¦Ù‚.")
#         return

#     print(f"âœ… Ø¹Ø¯Ø¯ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø© Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³: {len(candidate_doc_ids)}")
#     candidate_indices = [doc_id_to_idx[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_idx]

#     if not candidate_indices:
#         print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…ØªÙˆÙØ±Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚.")
#         return

#     # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø²Ù…Ù†
#     start_time = time.time()

#     query_vec = tfidf_vectorizer.transform([" ".join(tokens)])
#     sub_doc_matrix = doc_matrix[candidate_indices]
#     sims = cosine_similarity(query_vec, sub_doc_matrix)[0]

#     top_indices = np.argsort(sims)[::-1][:TOP_N]

#     end_time = time.time()
#     execution_time = end_time - start_time

#     print(f"\nðŸ“„ Ø£Ø¹Ù„Ù‰ {TOP_N} Ù†ØªØ§Ø¦Ø¬:\n")
#     for rank, i in enumerate(top_indices, 1):
#         doc_idx = candidate_indices[i]
#         print(f"{rank}. doc_id: {doc_ids[doc_idx]}, similarity: {sims[i]:.4f}")

#     print(f"\nâ±ï¸ Ø²Ù…Ù† ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {execution_time:.4f} Ø«Ø§Ù†ÙŠØ©")

# # Ø§Ù„Ø¨Ø­Ø«
# while True:
#     query = input("\nðŸ” [TF-IDF] Ø£Ø¯Ø®Ù„ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ùƒ (Ø£Ùˆ 'exit'): ").strip()
#     if query.lower() == 'exit':
#         break
#     search_tfidf(query)
from flask import Flask, request, jsonify
import joblib
import os
import sqlite3
import numpy as np
import time
import requests
from sklearn.metrics.pairwise import cosine_similarity

# Ø¥Ø¹Ø¯Ø§Ø¯ Flask
app = Flask(__name__)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
INDEX_DIR = 'indexes'
MODELS_DIR = 'models'
TOP_N = 10
DB_PATH = "ir_project.db"
PREPROCESS_API_URL = "http://127.0.0.1:5060/preprocess"
AVAILABLE_SOURCES = ["quora"]

# ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect(DB_PATH, check_same_thread=False)
cursor = conn.cursor()

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø±
resources = {}
for source in AVAILABLE_SOURCES:
    try:
        print(f"ðŸ”„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù„Ù€ {source}...")
        inverted_index = joblib.load(os.path.join(INDEX_DIR, f"inverted_index_{source}.joblib"))
        tfidf_vectorizer = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{source}_vectorizer.joblib"))
        doc_ids = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{source}_doc_ids.joblib"))
        doc_matrix = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{source}_matrix.joblib"))
        doc_id_to_idx = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}
        resources[source] = {
            "inverted_index": inverted_index,
            "vectorizer": tfidf_vectorizer,
            "doc_ids": doc_ids,
            "doc_matrix": doc_matrix,
            "doc_id_to_idx": doc_id_to_idx
        }
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØµØ¯Ø± {source}: {e}")

@app.route('/search-tfidf-inverted', methods=['POST'])
def tfidf_inverted_search():
    data = request.get_json()
    query = data.get("query", "").strip()
    source = data.get("source", "quora").strip().lower()

    if not query:
        return jsonify({"error": "âš ï¸ ÙŠØ¬Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙÙŠ Ø§Ù„Ø­Ù‚Ù„ 'query'"}), 400

    if source not in resources:
        return jsonify({"error": f"âš ï¸ Ø§Ù„Ù…ØµØ¯Ø± ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…. Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©: {AVAILABLE_SOURCES}"}), 400

    res = resources[source]

    # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ© Ø¹Ø¨Ø± Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
    try:
        response = requests.post(PREPROCESS_API_URL, json={
            "text": query,
            "return_as_string": False
        })
        if response.status_code != 200:
            return jsonify({"error": "âš ï¸ ÙØ´Ù„ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ©"}), 500

        tokens = response.json().get("tokens", [])
    except Exception as e:
        return jsonify({"error": f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"}), 500

    if not tokens:
        return jsonify({"error": "âš ï¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"}), 400

    inverted_index = res['inverted_index']
    tfidf_vectorizer = res['vectorizer']
    doc_ids = res['doc_ids']
    doc_matrix = res['doc_matrix']
    doc_id_to_idx = res['doc_id_to_idx']

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø±Ø´Ø­Ø©
    candidate_doc_ids = set()
    for term in tokens:
        if term in inverted_index:
            candidate_doc_ids.update(inverted_index[term])

    if not candidate_doc_ids:
        return jsonify({"results": [], "message": "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙˆØ«Ø§Ø¦Ù‚."})

    candidate_indices = [doc_id_to_idx[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_idx]

    if not candidate_indices:
        return jsonify({"results": [], "message": "âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙ…Ø«ÙŠÙ„Ø§Øª Ù…ØªØ§Ø­Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚."})

    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
    start_time = time.time()
    query_vec = tfidf_vectorizer.transform([" ".join(tokens)])
    sub_doc_matrix = doc_matrix[candidate_indices]
    sims = cosine_similarity(query_vec, sub_doc_matrix)[0]
    top_indices = np.argsort(sims)[::-1][:TOP_N]
    end_time = time.time()

    results = []
    for rank, i in enumerate(top_indices, 1):
        doc_idx = candidate_indices[i]
        doc_id = doc_ids[doc_idx]
        score = sims[i]

        cursor.execute("SELECT content FROM documents WHERE doc_id = ? AND source = ?", (doc_id, source))
        row = cursor.fetchone()
        content = row[0] if row else "(Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ)"

        results.append({
            "rank": rank,
            "doc_id": doc_id,
            "score": round(float(score), 4),
            "content": content[:300] + ("..." if len(content) > 300 else "")
        })

    return jsonify({
        "query": query,
        "tokens": tokens,
        "source": source,
        "execution_time": round(end_time - start_time, 4),
        "results": results
    })


if __name__ == '__main__':
    app.run(debug=True, port=5013)
