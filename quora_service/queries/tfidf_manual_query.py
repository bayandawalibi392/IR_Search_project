# import os
# import sqlite3
# import joblib
# import numpy as np
# import time
# import sys
# import os
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
# from sklearn.metrics.pairwise import cosine_similarity
# from text_preprocessing_service import TextPreprocessingService

# # --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# MODELS_DIR = "models"
# SOURCE = "quora"  # Ø£Ùˆ "quora"
# TOP_N = 10

# # Ø±Ø¨Ø· Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# conn = sqlite3.connect("ir_project.db")
# cursor = conn.cursor()

# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
# cursor.execute("SELECT query_id, query_text FROM queries WHERE source = ?", (SOURCE,))
# all_queries = cursor.fetchall()

# print(f"\nðŸ”Ž Ø§Ø®ØªØ± Ø§Ø³ØªØ¹Ù„Ø§Ù…Ù‹Ø§ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© {SOURCE.upper()}:")
# for i, (qid, qtext) in enumerate(all_queries[:10]):
#     print(f"{i+1}. {qtext}")

# index = int(input("\nðŸ“Œ Ø£Ø¯Ø®Ù„ Ø±Ù‚Ù… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (1-10): ")) - 1
# query_id, query_text = all_queries[index]

# print(f"\nðŸ§  ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query_text} (ID: {query_id})")

# # ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª TF-IDF
# vectorizer = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_vectorizer.joblib"))
# doc_ids = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_doc_ids.joblib"))
# doc_matrix = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_matrix.joblib"))

# # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
# preprocessor = TextPreprocessingService()
# cleaned_query = preprocessor.preprocess(query_text, return_as_string=True)

# # Ø¨Ø¯Ø¡ ØªÙˆÙ‚ÙŠØª Ø§Ù„ØªÙ†ÙÙŠØ°
# start_time = time.time()

# query_vec = vectorizer.transform([cleaned_query])
# sims = cosine_similarity(query_vec, doc_matrix)[0]
# top_indices = np.argsort(sims)[::-1][:TOP_N]

# # Ù†Ù‡Ø§ÙŠØ© ØªÙˆÙ‚ÙŠØª Ø§Ù„ØªÙ†ÙÙŠØ°
# end_time = time.time()
# execution_time = end_time - start_time

# # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
# print(f"\nðŸ“„ Ø£Ø¹Ù„Ù‰ {TOP_N} Ù†ØªØ§Ø¦Ø¬ Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ùƒ:\n")
# for rank, idx in enumerate(top_indices, 1):
#     doc_id = doc_ids[idx]
#     similarity = sims[idx]

#     cursor.execute("SELECT content FROM documents WHERE doc_id = ?", (doc_id,))
#     result = cursor.fetchone()
#     content = result[0] if result else "(Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ)"

#     print(f"{rank}. doc_id: {doc_id}")
#     print(f"   similarity: {similarity:.4f}")
#     print(f"   content: {content[:300]}...")
#     print("-" * 80)

# print(f"\nâ±ï¸ Ø²Ù…Ù† ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {execution_time:.4f} Ø«Ø§Ù†ÙŠØ©")
# conn.close()
from flask import Flask, request, jsonify
import os
import sqlite3
import joblib
import numpy as np
import time
import requests
from sklearn.metrics.pairwise import cosine_similarity

# Ø¥Ø¹Ø¯Ø§Ø¯ Flask
app = Flask(__name__)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
MODELS_DIR = "models"
SOURCE = "quora"
TOP_N = 10
DB_PATH = "ir_project.db"
PREPROCESS_API_URL = "http://127.0.0.1:5060/preprocess"

# ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª TF-IDF
vectorizer = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_vectorizer.joblib"))
doc_ids = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_doc_ids.joblib"))
doc_matrix = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_matrix.joblib"))

# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect(DB_PATH, check_same_thread=False)
cursor = conn.cursor()

@app.route('/search-tfidf', methods=['POST'])
def search_tfidf():
    data = request.get_json()
    query_text = data.get("query", "").strip()

    if not query_text:
        return jsonify({"error": "âš ï¸ ÙŠØ¬Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ø­Ù‚Ù„ 'query'"}), 400

    # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ© Ø¹Ø¨Ø± API
    try:
        response = requests.post(PREPROCESS_API_URL, json={
            "text": query_text,
            "return_as_string": True
        })
        if response.status_code != 200:
            return jsonify({"error": "âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ©"}), 500

        cleaned_query = response.json().get("clean_text", "")
    except Exception as e:
        return jsonify({"error": f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"}), 500

    # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø«
    start_time = time.time()
    query_vec = vectorizer.transform([cleaned_query])
    sims = cosine_similarity(query_vec, doc_matrix)[0]
    top_indices = np.argsort(sims)[::-1][:TOP_N]
    end_time = time.time()

    results = []
    for rank, idx in enumerate(top_indices, 1):
        doc_id = doc_ids[idx]
        similarity = sims[idx]

        cursor.execute("SELECT content FROM documents WHERE doc_id = ?", (doc_id,))
        row = cursor.fetchone()
        content = row[0] if row else "(Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ)"

        results.append({
            "rank": rank,
            "doc_id": doc_id,
            "score": round(float(similarity), 4),
            "content": content[:300] + ("..." if len(content) > 300 else "")
        })

    return jsonify({
        "query": query_text,
        "cleaned_query": cleaned_query,
        "results": results,
        "execution_time": round(end_time - start_time, 4)
    })

if __name__ == '__main__':
    app.run(port=5010, debug=True)
