import os
import sqlite3
import joblib
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
from text_preprocessing_service import TextPreprocessingService

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
MODELS_DIR = "models"
SOURCES = [ "quora"]
TOP_N = 5

# ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ BERT Ù†ÙØ³Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ (all-MiniLM-L6-v2)
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

def embed_text(text):
    """Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ embedding Ù„Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT"""
    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)
    with torch.no_grad():
        model_output = model(**encoded_input)
    # mean pooling
    embeddings = model_output.last_hidden_state.mean(dim=1).cpu().numpy()
    return embeddings

# Ø±Ø¨Ø· Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect("ir_project.db")
cursor = conn.cursor()

# Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ©
preprocessor = TextPreprocessingService()

for source in SOURCES:
    print(f"\nğŸ” Running BERT query matching for source: {source}")

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
    cursor.execute("SELECT query_id, query_text FROM queries WHERE source = ?", (source,))
    queries = cursor.fetchall()

    # ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª BERT embeddings Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚
    doc_ids = joblib.load(os.path.join(MODELS_DIR, f"bert_{source}_doc_ids.joblib"))
    doc_vectors = joblib.load(os.path.join(MODELS_DIR, f"bert_{source}_vectors.joblib"))  # numpy array (N_docs x embedding_dim)

    results = []  # Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (query_id, doc_id, similarity)

    for query_id, query_text in tqdm(queries, desc=f"Matching queries - {source}"):
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        cleaned_query = preprocessor.preprocess(query_text, return_as_string=True)
        # ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… BERT
        query_vec = embed_text(cleaned_query)  # Ø´ÙƒÙ„ (1, embedding_dim)

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (cosine similarity)
        sims = cosine_similarity(query_vec, doc_vectors)[0]
        top_indices = np.argsort(sims)[::-1][:TOP_N]

        for rank, idx in enumerate(top_indices):
            doc_id = doc_ids[idx]
            sim_score = sims[idx]
            results.append((query_id, doc_id, sim_score))

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    output_path = os.path.join(MODELS_DIR, f"bert_{source}_top{TOP_N}_results.tsv")
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("query_id\tdoc_id\tscore\n")
        for qid, did, score in results:
            f.write(f"{qid}\t{did}\t{score:.4f}\n")

    print(f"âœ… Saved top-{TOP_N} results for '{source}' to: {output_path}")

conn.close()
