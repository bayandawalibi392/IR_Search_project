# import os
# import sqlite3
# import joblib
# import numpy as np
# from sklearn.metrics.pairwise import cosine_similarity
# from nltk.tokenize import word_tokenize
# from rank_bm25 import BM25Okapi
# from text_preprocessing_service import TextPreprocessingService

# # --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# MODELS_DIR = "models"
# TOP_N = 10
# ALPHA = 0.5  # ÙˆØ²Ù† TF-IDF Ù…Ù‚Ø§Ø¨Ù„ BM25 (ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„)

# # Ø±Ø¨Ø· Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)
# conn = sqlite3.connect("ir_project.db")
# cursor = conn.cursor()

# available_sources = ['webis', 'quora']

# def perform_search(SOURCE):
#     cursor.execute("SELECT query_id, query_text FROM queries WHERE source = ?", (SOURCE,))
#     all_queries = cursor.fetchall()
#     if not all_queries:
#         print(f"Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù„Ù…ØµØ¯Ø± {SOURCE}")
#         return

#     print(f"\nğŸ” Ø§Ø®ØªØ± Ø§Ø³ØªØ¹Ù„Ø§Ù…Ù‹Ø§ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© {SOURCE.upper()}:")
#     for i, (qid, qtext) in enumerate(all_queries[:10]):
#         print(f"{i+1}. {qtext}")

#     try:
#         index = int(input("\nğŸ“Œ Ø£Ø¯Ø®Ù„ Ø±Ù‚Ù… Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (1-10): ")) - 1
#         if index < 0 or index >= len(all_queries):
#             print("Ø±Ù‚Ù… ØºÙŠØ± ØµØ­ÙŠØ­ØŒ Ø­Ø§ÙˆÙ„ Ù…Ø¬Ø¯Ø¯Ø§Ù‹.")
#             return
#     except ValueError:
#         print("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ù‚Ù… ØµØ­ÙŠØ­.")
#         return

#     query_id, query_text = all_queries[index]
#     print(f"\nğŸ§  ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query_text} (ID: {query_id})")

#     # Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ©
#     preprocessor = TextPreprocessingService()
#     cleaned_query = preprocessor.preprocess(query_text, return_as_string=True)

#     # --- ØªØ­Ù…ÙŠÙ„ ØªÙ…Ø«ÙŠÙ„Ø§Øª TF-IDF ---
#     tfidf_vectorizer = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_vectorizer.joblib"))
#     tfidf_doc_ids = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_doc_ids.joblib"))
#     tfidf_matrix = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_matrix.joblib"))

#     tfidf_query_vec = tfidf_vectorizer.transform([cleaned_query])
#     sims_tfidf = cosine_similarity(tfidf_query_vec, tfidf_matrix)[0]

#     # --- ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ BM25 ---
#     bm25_data = joblib.load(os.path.join(MODELS_DIR, f"bm25_{SOURCE}_model.joblib"))
#     bm25_doc_ids = bm25_data['doc_ids']
#     tokenized_docs = bm25_data['tokenized_texts']
#     bm25 = BM25Okapi(tokenized_docs, k1=bm25_data['k1'], b=bm25_data['b'])

#     # ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù„Ù€ BM25
#     tokenized_query = word_tokenize(cleaned_query)
#     sims_bm25 = bm25.get_scores(tokenized_query)

#     # --- Ù…ÙˆØ§Ø¦Ù…Ø© Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ ---
#     tfidf_id_to_idx = {doc_id: i for i, doc_id in enumerate(tfidf_doc_ids)}
#     bm25_id_to_idx = {doc_id: i for i, doc_id in enumerate(bm25_doc_ids)}

#     common_doc_ids = list(set(tfidf_doc_ids).intersection(set(bm25_doc_ids)))

#     tfidf_indices = [tfidf_id_to_idx[doc_id] for doc_id in common_doc_ids]
#     bm25_indices = [bm25_id_to_idx[doc_id] for doc_id in common_doc_ids]

#     aligned_sims_tfidf = sims_tfidf[tfidf_indices]
#     aligned_sims_bm25 = np.array([sims_bm25[i] for i in bm25_indices])

#     # --- Ø¯Ù…Ø¬ Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª ---
#     final_sims = ALPHA * aligned_sims_tfidf + (1 - ALPHA) * aligned_sims_bm25

#     top_indices = np.argsort(final_sims)[::-1][:TOP_N]

#     # --- Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ---
#     print(f"\nğŸ“„ Ø£Ø¹Ù„Ù‰ {TOP_N} Ù†ØªØ§Ø¦Ø¬ (ØªÙ…Ø«ÙŠÙ„ Ù‡Ø¬ÙŠÙ† Ù…ØªÙˆØ§Ø²ÙŠ TF-IDF + BM25ØŒ ALPHA = {ALPHA}):\n")
#     for rank, idx in enumerate(top_indices, 1):
#         doc_id = common_doc_ids[idx]
#         score = final_sims[idx]

#         cursor.execute("SELECT content FROM documents WHERE doc_id = ?", (doc_id,))
#         result = cursor.fetchone()
#         content = result[0] if result else "(Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ)"

#         print(f"{rank}. doc_id: {doc_id}")
#         print(f"   hybrid_similarity: {score:.4f}")
#         print(f"   content: {content[:300]}...")
#         print("-" * 80)

# def main_loop():
#     print("Ø§ÙƒØªØ¨ Exit Ù„Ù„Ø®Ø±ÙˆØ¬ ÙÙŠ Ø£ÙŠ ÙˆÙ‚Øª.\n")
#     while True:
#         print("Available sources:")
#         for i, src in enumerate(available_sources, 1):
#             print(f"{i}. {src}")
#         user_input = input("Select a source by number or type Exit to quit: ").strip()
#         if user_input.lower() == "exit":
#             print("Ø®Ø±ÙˆØ¬ Ù…Ù† Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬.")
#             break
#         try:
#             selected_idx = int(user_input) - 1
#             if selected_idx < 0 or selected_idx >= len(available_sources):
#                 print("Ø±Ù‚Ù… ØºÙŠØ± ØµØ­ÙŠØ­ØŒ Ø­Ø§ÙˆÙ„ Ù…Ø¬Ø¯Ø¯Ø§Ù‹.\n")
#                 continue
#         except ValueError:
#             print("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ù‚Ù… ØµØ­ÙŠØ­ Ø£Ùˆ Exit.\n")
#             continue

#         source = available_sources[selected_idx]
#         perform_search(source)
#         print("\n" + "="*50 + "\n")

# if __name__ == "__main__":
#     main_loop()
#     conn.close()

from flask import Flask, request, jsonify
import os
import sqlite3
import joblib
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from nltk.tokenize import word_tokenize
from rank_bm25 import BM25Okapi
import requests

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
MODELS_DIR = "models"
TOP_N = 10
ALPHA = 0.5  # ÙˆØ²Ù† TF-IDF Ù…Ù‚Ø§Ø¨Ù„ BM25
PREPROCESS_API_URL = "http://127.0.0.1:5060/preprocess"  # Ø±Ø§Ø¨Ø· Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©

# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©
available_sources = ['quora']

# Ø¥Ù†Ø´Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚ Flask
app = Flask(__name__)

# ÙØªØ­ Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ù† Ø¹Ø¯Ø© Ø«Ø±ÙŠØ¯Ø§Øª
conn = sqlite3.connect("ir_project.db", check_same_thread=False)
cursor = conn.cursor()

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ…ÙˆØ§Ø±Ø¯ Ù„ÙƒÙ„ Ù…ØµØ¯Ø± Ø¹Ù†Ø¯ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
resources = {}
for SOURCE in available_sources:
    try:
        # ØªØ­Ù…ÙŠÙ„ TF-IDF
        tfidf_vectorizer = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_vectorizer.joblib"))
        tfidf_doc_ids = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_doc_ids.joblib"))
        tfidf_matrix = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_matrix.joblib"))

        # ØªØ­Ù…ÙŠÙ„ BM25
        bm25 = joblib.load(os.path.join(MODELS_DIR, f"bm25_{SOURCE}_model.joblib"))
        # bm25_doc_ids = bm25_data['doc_ids']
        bm25_doc_ids = joblib.load(os.path.join(MODELS_DIR, f"bm25_tokenized_docs_{group}.joblib"))

        tokenized_docs = bm25_data['tokenized_texts']
        bm25 = BM25Okapi(tokenized_docs, k1=bm25_data['k1'], b=bm25_data['b'])

        resources[SOURCE] = {
            "tfidf_vectorizer": tfidf_vectorizer,
            "tfidf_doc_ids": tfidf_doc_ids,
            "tfidf_matrix": tfidf_matrix,
            "bm25": bm25,
            "bm25_doc_ids": bm25_doc_ids
        }
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù„Ù„Ù…ØµØ¯Ø±: {SOURCE}")
    except Exception as e:
        print(f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ù„Ù„Ù…ØµØ¯Ø± {SOURCE}: {e}")

@app.route("/search-hybrid", methods=["POST"])
def search_hybrid():
    data = request.get_json()
    query = data.get("query", "").strip()
    source = data.get("source", "quora").strip().lower()

    if not query:
        return jsonify({"error": "âš ï¸ ÙŠØ¬Ø¨ Ø¥Ø±Ø³Ø§Ù„ Ø­Ù‚Ù„ 'query' ÙÙŠ Ø§Ù„Ø·Ù„Ø¨"}), 400
    if source not in resources:
        return jsonify({"error": f"âš ï¸ Ø§Ù„Ù…ØµØ¯Ø± ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…. Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©: {available_sources}"}), 400

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
    try:
        response = requests.post(PREPROCESS_API_URL, json={
            "text": query,
            "return_as_string": True  # Ù†Øµ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ†Øµ ÙˆØ§Ø­Ø¯
        })
        if response.status_code != 200:
            return jsonify({"error": "âš ï¸ ÙØ´Ù„ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©"}), 500
        cleaned_query = response.json().get("clean_text", "")
    except Exception as e:
        return jsonify({"error": f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}"}), 500

    if not cleaned_query.strip():
        return jsonify({"error": "âš ï¸ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙØ§Ø±Øº Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"}), 400

    # Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù…ØµØ¯Ø±
    tfidf_vectorizer = resources[source]["tfidf_vectorizer"]
    tfidf_doc_ids = resources[source]["tfidf_doc_ids"]
    tfidf_matrix = resources[source]["tfidf_matrix"]
    bm25 = resources[source]["bm25"]
    bm25_doc_ids = resources[source]["bm25_doc_ids"]

    # ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¨Ù€ TF-IDF
    tfidf_query_vec = tfidf_vectorizer.transform([cleaned_query])
    sims_tfidf = cosine_similarity(tfidf_query_vec, tfidf_matrix)[0]

    # ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù„Ù€ BM25
    tokenized_query = word_tokenize(cleaned_query)
    sims_bm25 = bm25.get_scores(tokenized_query)

    # Ù…ÙˆØ§Ø¦Ù…Ø© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ© Ø¨ÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ†
    tfidf_id_to_idx = {doc_id: i for i, doc_id in enumerate(tfidf_doc_ids)}
    bm25_id_to_idx = {doc_id: i for i, doc_id in enumerate(bm25_doc_ids)}

    common_doc_ids = list(set(tfidf_doc_ids).intersection(set(bm25_doc_ids)))

    tfidf_indices = [tfidf_id_to_idx[doc_id] for doc_id in common_doc_ids]
    bm25_indices = [bm25_id_to_idx[doc_id] for doc_id in common_doc_ids]

    aligned_sims_tfidf = sims_tfidf[tfidf_indices]
    aligned_sims_bm25 = np.array([sims_bm25[i] for i in bm25_indices])

    # Ø¯Ù…Ø¬ Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª
    final_sims = ALPHA * aligned_sims_tfidf + (1 - ALPHA) * aligned_sims_bm25
    top_indices = np.argsort(final_sims)[::-1][:TOP_N]

    results = []
    for rank, idx in enumerate(top_indices, 1):
        doc_id = common_doc_ids[idx]
        score = final_sims[idx]

        cursor.execute("SELECT content FROM documents WHERE doc_id = ?", (doc_id,))
        result = cursor.fetchone()
        content = result[0] if result else "(Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ)"

        results.append({
            "rank": rank,
            "doc_id": doc_id,
            "hybrid_similarity": round(float(score), 4),
            "content": content[:500]
        })

    return jsonify({
        "query": query,
        "cleaned_query": cleaned_query,
        "source": source,
        "top_n": TOP_N,
        "results": results
    })


if __name__ == "__main__":
    app.run(port=5018, debug=True)
