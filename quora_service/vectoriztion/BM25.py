import os
import sqlite3
import joblib
import nltk
from nltk.tokenize import word_tokenize
from rank_bm25 import BM25Okapi
from tqdm import tqdm

nltk.download('punkt')

MODELS_DIR = "models"
SOURCES = ['webis', 'quora']
os.makedirs(MODELS_DIR, exist_ok=True)

DEFAULT_K1 = 1.5
DEFAULT_B = 0.75

conn = sqlite3.connect("ir_project.db")
cursor = conn.cursor()

for source in SOURCES:
    print(f"ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØµØ¯Ø±: {source}")

    cursor.execute("SELECT doc_id, content FROM preprocessed_documents WHERE source = ?", (source,))
    rows = cursor.fetchall()
    if not rows:
        print(f"âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„Ù…ØµØ¯Ø± {source}ØŒ ØªØ®Ø·ÙŠ...")
        continue

    doc_ids, contents = zip(*rows)
    print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª: {len(contents)}")

    print(f"Ø¨Ø¯Ø¡ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù„Ù€ {source}...")
    tokenized_corpus = [word_tokenize(doc) for doc in tqdm(contents, desc=f"Tokenizing {source} docs")]
    print(f"Ø§Ù†ØªÙ‡Ù‰ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù€ {source}.")

    print(f"Ø¨Ø¯Ø¡ Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ BM25 Ù„Ù€ {source}...")
    bm25 = BM25Okapi(tokenized_corpus, k1=DEFAULT_K1, b=DEFAULT_B)
    print(f"ØªÙ… Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ BM25 Ù„Ù€ {source}.")

    data_to_save = {
        'doc_ids': doc_ids,
        'tokenized_texts': tokenized_corpus,
        'k1': DEFAULT_K1,
        'b': DEFAULT_B
    }

    save_path = os.path.join(MODELS_DIR, f"bm25_{source}_model.joblib")
    joblib.dump(data_to_save, save_path)
    print(f"âœ… ØªÙ… Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ BM25 Ù„Ù„Ù…ØµØ¯Ø± {source} ÙÙŠ {save_path}\n")

conn.close()
print("Ø§Ù†ØªÙ‡Ù‰ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ­ÙØ¸ Ù†Ù…Ø§Ø°Ø¬ BM25 Ù„ÙƒÙ„ Ø§Ù„Ù…ØµØ§Ø¯Ø±.")
