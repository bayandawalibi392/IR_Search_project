# import os
# import sqlite3
# import joblib
# import numpy as np
# from sklearn.metrics import average_precision_score
# from sklearn.metrics.pairwise import cosine_similarity
# from transformers import AutoTokenizer, AutoModel
# import torch
# from tqdm import tqdm
# from text_preprocessing_service import TextPreprocessingService

# # --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# SOURCE = "quora"  # Ø£Ùˆ "quora"
# MODELS_DIR = "models"
# INDEX_DIR = "indexes"
# TOP_N = 10
# ALPHA = 0.6  # ÙˆØ²Ù† TF-IDF Ù…Ù‚Ø§Ø¨Ù„ BERT

# # ØªØ­Ù…ÙŠÙ„ BERT
# tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
# model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = model.to(device)
# model.eval()

# def embed_text(text):
#     encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)
#     with torch.no_grad():
#         output = model(**encoded_input)
#     return output.last_hidden_state.mean(dim=1).cpu().numpy()

# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³
# inverted_index = joblib.load(os.path.join(INDEX_DIR, f"inverted_index_{SOURCE}.joblib"))

# # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª
# tfidf_vectorizer = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_vectorizer.joblib"))
# tfidf_doc_ids = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_doc_ids.joblib"))
# tfidf_matrix = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_matrix.joblib"))

# bert_doc_ids = joblib.load(os.path.join(MODELS_DIR, f"bert_{SOURCE}_doc_ids.joblib"))
# bert_vectors = joblib.load(os.path.join(MODELS_DIR, f"bert_{SOURCE}_vectors.joblib"))

# tfidf_id_to_idx = {doc_id: i for i, doc_id in enumerate(tfidf_doc_ids)}
# bert_id_to_idx = {doc_id: i for i, doc_id in enumerate(bert_doc_ids)}

# # Ù…Ø¹Ø§Ù„Ø¬Ø©
# preprocessor = TextPreprocessingService()

# # Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# conn = sqlite3.connect("ir_project.db")
# cursor = conn.cursor()

# cursor.execute("SELECT query_id, query_text FROM queries WHERE source = ?", (SOURCE,))
# queries = cursor.fetchall()

# cursor.execute("SELECT query_id, doc_id FROM qrels WHERE source = ?", (SOURCE,))
# qrels_raw = cursor.fetchall()

# # qrels ÙƒÙ‚Ø§Ù…ÙˆØ³
# qrels = {}
# for qid, doc_id in qrels_raw:
#     qrels.setdefault(qid, set()).add(doc_id)

# # --- ØªÙ‚ÙŠÙŠÙ… ---
# precisions, recalls, average_precisions, reciprocal_ranks = [], [], [], []

# print(f"\nâš™ï¸ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ {len(queries)} Ø§Ø³ØªØ¹Ù„Ø§Ù…...\n")

# for qid, query_text in tqdm(queries):
#     if qid not in qrels:
#         continue

#     relevant_docs = qrels[qid]
#     tokens = preprocessor.preprocess(query_text, return_as_string=False)
#     cleaned_query = preprocessor.preprocess(query_text, return_as_string=True)

#     # Ù…Ø±Ø´Ø­ÙˆÙ† Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³
#     candidate_doc_ids = set()
#     for token in tokens:
#         if token in inverted_index:
#             candidate_doc_ids.update(inverted_index[token])

#     common_doc_ids = list(candidate_doc_ids.intersection(tfidf_doc_ids).intersection(bert_doc_ids))
#     if not common_doc_ids:
#         continue

#     tfidf_indices = [tfidf_id_to_idx[doc_id] for doc_id in common_doc_ids]
#     bert_indices = [bert_id_to_idx[doc_id] for doc_id in common_doc_ids]

#     # Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª
#     tfidf_query_vec = tfidf_vectorizer.transform([cleaned_query])
#     sims_tfidf = cosine_similarity(tfidf_query_vec, tfidf_matrix[tfidf_indices])[0]

#     bert_query_vec = embed_text(cleaned_query)
#     sims_bert = cosine_similarity(bert_query_vec, bert_vectors[bert_indices])[0]

#     # Ø¯Ù…Ø¬ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª
#     final_sims = ALPHA * sims_tfidf + (1 - ALPHA) * sims_bert
#     ranked = [(doc_id, final_sims[i]) for i, doc_id in enumerate(common_doc_ids)]
#     ranked.sort(key=lambda x: x[1], reverse=True)
#     top_docs = [doc_id for doc_id, _ in ranked[:TOP_N]]

#     # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³
#     hits = [1 if doc in relevant_docs else 0 for doc in top_docs]
#     precisions.append(sum(hits) / TOP_N)
#     recalls.append(sum(hits) / len(relevant_docs))

#     # MAP
#     y_true = [1 if doc in relevant_docs else 0 for doc in common_doc_ids]
#     y_scores = final_sims
#     try:
#         ap = average_precision_score(y_true, y_scores)
#     except:
#         ap = 0.0
#     average_precisions.append(ap)

#     # MRR
#     for rank, doc in enumerate(top_docs, 1):
#         if doc in relevant_docs:
#             reciprocal_ranks.append(1 / rank)
#             break
#     else:
#         reciprocal_ranks.append(0.0)

# # --- Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ---
# print("\nğŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‡Ø¬ÙŠÙ† Ù…Ø¹ Ø§Ù„ÙÙ‡Ø±Ø³:")
# print(f"Precision@10: {np.mean(precisions):.4f}")
# print(f"Recall:        {np.mean(recalls):.4f}")
# print(f"MAP:           {np.mean(average_precisions):.4f}")
# print(f"MRR:           {np.mean(reciprocal_ranks):.4f}")

# conn.close()
import os
import sqlite3
import joblib
import numpy as np
import pandas as pd
import time
from sklearn.metrics import average_precision_score
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
import json
from text_preprocessing_service import TextPreprocessingService

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
SOURCE = "quora"
MODELS_DIR = "models"
INDEX_DIR = "indexes"
TOP_N = 10
ALPHA = 0.6  # ÙˆØ²Ù† TF-IDF Ù…Ù‚Ø§Ø¨Ù„ BERT

# ØªØ­Ù…ÙŠÙ„ BERT
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
model.eval()

def embed_text(text):
    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)
    with torch.no_grad():
        output = model(**encoded_input)
    return output.last_hidden_state.mean(dim=1).cpu().numpy()

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø§Ø±Ø³ ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬
inverted_index = joblib.load(os.path.join(INDEX_DIR, f"inverted_index_{SOURCE}.joblib"))
tfidf_vectorizer = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_vectorizer.joblib"))
tfidf_doc_ids = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_doc_ids.joblib"))
tfidf_matrix = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_matrix.joblib"))
bert_doc_ids = joblib.load(os.path.join(MODELS_DIR, f"bert_{SOURCE}_doc_ids.joblib"))
bert_vectors = joblib.load(os.path.join(MODELS_DIR, f"bert_{SOURCE}_vectors.joblib"))
tfidf_id_to_idx = {doc_id: i for i, doc_id in enumerate(tfidf_doc_ids)}
bert_id_to_idx = {doc_id: i for i, doc_id in enumerate(bert_doc_ids)}

# Ù…Ø¹Ø§Ù„Ø¬Ø©
preprocessor = TextPreprocessingService()

# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect("ir_project.db")
cursor = conn.cursor()
cursor.execute("SELECT query_id, query_text FROM queries WHERE source = ?", (SOURCE,))
queries = cursor.fetchall()
cursor.execute("SELECT query_id, doc_id FROM qrels WHERE source = ?", (SOURCE,))
qrels_raw = cursor.fetchall()

# qrels ÙƒÙ‚Ø§Ù…ÙˆØ³
qrels = {}
for qid, doc_id in qrels_raw:
    qrels.setdefault(qid, set()).add(doc_id)

# --- ØªÙ‚ÙŠÙŠÙ… ---
precisions, recalls, average_precisions, reciprocal_ranks = [], [], [], []
start_time = time.perf_counter()

print(f"\nâš™ï¸ Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¹Ù„Ù‰ {len(queries)} Ø§Ø³ØªØ¹Ù„Ø§Ù…...\n")

for qid, query_text in tqdm(queries):
    if qid not in qrels:
        continue

    relevant_docs = qrels[qid]
    tokens = preprocessor.preprocess(query_text, return_as_string=False)
    cleaned_query = preprocessor.preprocess(query_text, return_as_string=True)

    candidate_doc_ids = set()
    for token in tokens:
        if token in inverted_index:
            candidate_doc_ids.update(inverted_index[token])

    common_doc_ids = list(candidate_doc_ids.intersection(tfidf_doc_ids).intersection(bert_doc_ids))
    if not common_doc_ids:
        continue

    tfidf_indices = [tfidf_id_to_idx[doc_id] for doc_id in common_doc_ids]
    bert_indices = [bert_id_to_idx[doc_id] for doc_id in common_doc_ids]

    tfidf_query_vec = tfidf_vectorizer.transform([cleaned_query])
    sims_tfidf = cosine_similarity(tfidf_query_vec, tfidf_matrix[tfidf_indices])[0]

    bert_query_vec = embed_text(cleaned_query)
    sims_bert = cosine_similarity(bert_query_vec, bert_vectors[bert_indices])[0]

    final_sims = ALPHA * sims_tfidf + (1 - ALPHA) * sims_bert
    ranked = [(doc_id, final_sims[i]) for i, doc_id in enumerate(common_doc_ids)]
    ranked.sort(key=lambda x: x[1], reverse=True)
    top_docs = [doc_id for doc_id, _ in ranked[:TOP_N]]

    hits = [1 if doc in relevant_docs else 0 for doc in top_docs]
    precisions.append(sum(hits) / TOP_N)
    recalls.append(sum(hits) / len(relevant_docs))

    y_true = [1 if doc in relevant_docs else 0 for doc in common_doc_ids]
    y_scores = final_sims
    try:
        ap = average_precision_score(y_true, y_scores)
    except:
        ap = 0.0
    average_precisions.append(ap)

    for rank, doc in enumerate(top_docs, 1):
        if doc in relevant_docs:
            reciprocal_ranks.append(1 / rank)
            break
    else:
        reciprocal_ranks.append(0.0)

end_time = time.perf_counter()
elapsed_time = round(end_time - start_time, 2)

# --- Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ---
results = {
    "Precision@10": round(np.mean(precisions), 4),
    "Recall": round(np.mean(recalls), 4),
    "MAP": round(np.mean(average_precisions), 4),
    "MRR": round(np.mean(reciprocal_ranks), 4),
    "Execution Time (seconds)": elapsed_time,
    "Queries Evaluated": len(precisions)
}

pd.DataFrame([results])

# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
results_df.to_csv("hybrid_evaluation_results.csv", index=False)
with open("hybrid_evaluation_results.json", "w", encoding="utf-8") as f:
    json.dump(results, f, indent=4, ensure_ascii=False)

# Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø§ØªØµØ§Ù„
conn.close()
