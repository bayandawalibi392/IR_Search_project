import joblib
import os
import sqlite3
import numpy as np
import pandas as pd
import time
from sklearn.metrics.pairwise import cosine_similarity
from text_preprocessing_service import TextPreprocessingService
import json

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
INDEX_DIR = 'indexes'
MODELS_DIR = 'models'
SOURCE = "quora"
TOP_N = 10

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¹ÙƒÙˆØ³
inverted_index = joblib.load(os.path.join(INDEX_DIR, f"inverted_index_{SOURCE}.joblib"))

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ TF-IDF
tfidf_vectorizer = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_vectorizer.joblib"))
doc_ids = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_doc_ids.joblib"))
doc_matrix = joblib.load(os.path.join(MODELS_DIR, f"tfidf_{SOURCE}_matrix.joblib"))
doc_id_to_idx = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}

# Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙŠØ©
preprocessor = TextPreprocessingService()

# Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
conn = sqlite3.connect("ir_project.db")
cursor = conn.cursor()

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª
cursor.execute("SELECT query_id, query_text FROM queries WHERE source = ?", (SOURCE,))
queries = cursor.fetchall()

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© (qrels)
cursor.execute("SELECT query_id, doc_id FROM qrels WHERE source = ?", (SOURCE,))
relevance_data = cursor.fetchall()

# ØªØ­ÙˆÙŠÙ„ qrels Ø¥Ù„Ù‰ dict: {query_id: set(doc_ids)}
relevant_docs = {}
for qid, did in relevance_data:
    relevant_docs.setdefault(qid, set()).add(did)

# Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
total_precisions = []
total_recalls = []
average_precisions = []
reciprocal_ranks = []

start_time = time.perf_counter()

for query_id, query_text in queries:
    tokens = preprocessor.preprocess(query_text, return_as_string=False)

    candidate_doc_ids = set()
    for term in tokens:
        if term in inverted_index:
            candidate_doc_ids.update(inverted_index[term])

    candidate_indices = [doc_id_to_idx[doc_id] for doc_id in candidate_doc_ids if doc_id in doc_id_to_idx]
    if not candidate_indices:
        continue

    query_vec = tfidf_vectorizer.transform([" ".join(tokens)])
    sims = cosine_similarity(query_vec, doc_matrix[candidate_indices])[0]
    top_indices = np.argsort(sims)[::-1][:TOP_N]
    retrieved_docs = [doc_ids[candidate_indices[i]] for i in top_indices]

    # ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
    rel_docs = relevant_docs.get(query_id, set())
    if not rel_docs:
        continue

    # Ø­Ø³Ø§Ø¨ Precision@10
    retrieved_relevant = [doc_id for doc_id in retrieved_docs if doc_id in rel_docs]
    precision_at_10 = len(retrieved_relevant) / TOP_N
    recall = len(retrieved_relevant) / len(rel_docs)

    # MRR
    rr = 0
    for rank, doc_id in enumerate(retrieved_docs, 1):
        if doc_id in rel_docs:
            rr = 1 / rank
            break

    # Average Precision
    num_rel = 0
    sum_precisions = 0
    for rank, doc_id in enumerate(retrieved_docs, 1):
        if doc_id in rel_docs:
            num_rel += 1
            sum_precisions += num_rel / rank
    ap = sum_precisions / len(rel_docs)

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
    total_precisions.append(precision_at_10)
    total_recalls.append(recall)
    reciprocal_ranks.append(rr)
    average_precisions.append(ap)

end_time = time.perf_counter()
elapsed = end_time - start_time

# Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·Ø§Øª
MAP = np.mean(average_precisions)
MRR = np.mean(reciprocal_ranks)
mean_precision = np.mean(total_precisions)
mean_recall = np.mean(total_recalls)

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ dict
results = {
    "Precision@10": round(mean_precision, 4),
    "Recall": round(mean_recall, 4),
    "MAP": round(MAP, 4),
    "MRR": round(MRR, 4),
    "Execution Time (seconds)": round(elapsed, 2),
    "Queries Evaluated": len(total_precisions)
}

# Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DataFrame
results_df = pd.DataFrame([results])
print("\nğŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ø¸Ø§Ù…:")
print(results_df)

# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ€ JSON
with open("tfidf_evaluation_results.json", "w", encoding="utf-8") as f:
    json.dump(results, f, indent=4, ensure_ascii=False)

# Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ€ CSV
results_df.to_csv("tfidf_evaluation_results.csv", index=False)

conn.close()
